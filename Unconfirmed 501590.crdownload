#!/usr/bin/env python
# coding: utf-8

# # Entity Frequency of Tweets
# 
# 
# ### Note: For analysis only first 50 rows were taken as a sample data due to technical issues while loading data.

# ## Importing Libraries

# In[1]:


import pandas as pd
import numpy as np
import json
import re
import nltk
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
import plotly.io as pio
from nltk import word_tokenize,pos_tag


# ##  Loading and cleaning data

# In[2]:


df = pd.read_json('tweets.json')


# In[3]:


df_1 = df.drop(df.index[[0]])


# In[4]:


df_2 = df_1.T


# In[5]:


df_2


# In[6]:


df_2.reset_index(level=0, inplace=True)


# In[7]:


df_3 = df_2.drop(['index'],axis=1)


# In[8]:


df_3


# In[9]:


new_df = df_3.head(51)


# In[10]:


new_df


# ## converting dataset to list and then to text
# 

# In[11]:


tweet_list = new_df['tweet_text'].to_list()
soup_text = " ".join(tweet_list)


# ## reading text

# In[12]:


ready_text = soup_text.lower()
ready_text = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))', '', ready_text, flags=re.MULTILINE)


# In[13]:


ready_text = ready_text.replace("cll"," ")
ready_text = ready_text.replace("\\n"," ")
ready_text = re.sub(r"[^a-zA]+", ' ', ready_text)


# In[14]:


nltk.download('stopwords')


# ## Tokenizing the text

# In[15]:


tokens = []
for t in ready_text.split():
    tokens.append(t)


# ## Optimizing the text using stopwords

# In[16]:


stop_words = stopwords.words('english')
clean_tokens = tokens[:]
for token in tokens:
    if token in stop_words:
        clean_tokens.remove(token)


# ## Further optimizing the tokens 

# In[17]:


def funcname(clean_tokens):
    for i in clean_tokens[:]:
            if len(i) <= 4:
                clean_tokens.remove(i)
            print(clean_tokens)
    return clean_tokens

clean_tokens = funcname(clean_tokens)


# In[18]:


nltk.download('averaged_perceptron_tagger')


# ## Parts of speech tagging the tokens 

# In[19]:


tag=pos_tag(clean_tokens)


# In[20]:


freq = nltk.FreqDist(tag)
for key, val in freq.items():
    print('Word: ' + str(key) + ', Quantity:' + str(val))


# ## Entities with high frquency of occurance

# In[21]:


high_freq = []
for key, val in freq.items():
    if (val > 5):
        high_freq.append({"entity":key[0],"POS":key[1], "frequency":val})


# In[22]:


print(high_freq)
freq_data = pd.DataFrame.from_dict(high_freq)


# In[23]:


freq_list= freq_data.sort_values(by=['frequency'],ascending = False)


# In[24]:


freq_list


# ## objective_1.csv

# In[25]:


freq_list.to_csv('objective_1')


# In[26]:


x = freq_list['entity']
y = freq_list['frequency']
plt.bar(x,y)
plt.title('Entity vs Frequency')
plt.xlabel('entity',fontsize = 25)
plt.ylabel('frequency',fontsize = 25)
plt.rcParams["figure.figsize"] = (45,30)
plt.show()


# ## wordcloud to visualize the entity frequency

# In[27]:


from wordcloud import WordCloud, STOPWORDS
from PIL import Image


# In[28]:


wordcloud = WordCloud(width = 800, height = 800,background_color ='white',min_font_size = 10).generate(str(clean_tokens))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.savefig('wordcloud11.png')
plt.show()


# # Sentiment Analysis 

# ## NLTK for sentiment analysis

# In[29]:


import nltk
nltk.download('vader_lexicon')


# In[30]:


df_4 = df.T


# In[31]:


df_5 = df_4.head(51)


# In[32]:


df_5


# In[33]:


df_6 = df_5.reset_index(inplace=False)


# In[34]:


df_7 = df_6.drop(columns=['index'])


# ## SentimentIntensityAnalyzer for obtainig the polarity score

# In[35]:


from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA

results = []

for tweet_text in new_df['tweet_text']:
    pol_score = SIA().polarity_scores(tweet_text) # run analysis
    pol_score['tweet_text'] = tweet_text # add headlines for viewing
    results.append(pol_score)


# In[36]:


results_data =pd.DataFrame(results) 
data_df = results_data.drop(columns=['neg','neu','pos'])


# In[37]:


score_data = pd.merge(df_7,data_df, on=['tweet_text'], how='outer')


# In[38]:


score_data


# ## objective_2.csv

# In[39]:


score_data.to_csv('objective_2')

